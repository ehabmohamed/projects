{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentiment Analysis using Bag-of-Words Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented using the <a href=\"https://www.crowdflower.com/data-for-everyone/\">Disasters on social media</a> dataset from Crowdflower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10855</th>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10856</th>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10857</th>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10858</th>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10859</th>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>Relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet     class\n",
       "10855  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...  Relevant\n",
       "10856  Police investigating after an e-bike collided ...  Relevant\n",
       "10857  The Latest: More Homes Razed by Northern Calif...  Relevant\n",
       "10858  MEG issues Hazardous Weather Outlook (HWO) htt...  Relevant\n",
       "10859  #CityofCalgary has activated its Municipal Eme...  Relevant"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('socialmedia-disaster-tweets-DFE.csv')[['text','choose_one']]\n",
    "tweets.columns = ['tweet','class']\n",
    "tweets = tweets[(tweets['class'] == 'Relevant') | (tweets['class'] == 'Not Relevant')]\n",
    "tweets = tweets.reset_index(drop=True)\n",
    "tweets.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove any HTML and Emoticons from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets['tweet'] = tweets['tweet'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reindex the tweets dataframe so the rows are in random order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   tweet         class\n",
      "10647  cutting for some celebrety and then posting th...  Not Relevant\n",
      "6758   heavy rain gusty winds and vivid lightning mov...      Relevant\n",
      "7748   follow up at 4700 block of sw 11th st gresham ...      Relevant\n",
      "3438   dozens die as two trains derail into a river i...      Relevant\n",
      "9783   billionaire mottas try getting trapped money o...  Not Relevant\n",
      "                                                   tweet         class\n",
      "7094   mike magner discusses a trust betrayed http t ...  Not Relevant\n",
      "8815                        ik4len sirens was cancelled   Not Relevant\n",
      "9940   i had trouble breathing while listening to kia...  Not Relevant\n",
      "10189  2012 shell s 250 foot tall drilling rig broke ...      Relevant\n",
      "1193                          eddietrunk blizzard of ozz      Relevant\n"
     ]
    }
   ],
   "source": [
    "tweets = tweets.reindex(np.random.permutation(tweets.index))\n",
    "\n",
    "print tweets.head()\n",
    "print tweets.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download stopwords from the NLTK, remove them from the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_into_tokens(tweet):\n",
    "    tweet = unicode(tweet, 'utf8')  # convert bytes into proper unicode\n",
    "    return TextBlob(tweet).words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10647    [cutting, for, some, celebrety, and, then, pos...\n",
       "6758     [heavy, rain, gusty, winds, and, vivid, lightn...\n",
       "7748     [follow, up, at, 4700, block, of, sw, 11th, st...\n",
       "3438     [dozens, die, as, two, trains, derail, into, a...\n",
       "9783     [billionaire, mottas, try, getting, trapped, m...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.tweet.head().apply(split_into_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', u'JJ'),\n",
       " ('world', u'NN'),\n",
       " ('how', u'WRB'),\n",
       " ('is', u'VBZ'),\n",
       " ('it', u'PRP'),\n",
       " ('going', u'VBG')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"hello world, how is it going?\").tags  # list of (word, POS) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dsg191\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop = stop + [u'a',u'b',u'c',u'd',u'e',u'f',u'g',u'h',u'i',u'j',u'k',u'l',u'm',u'n',u'o',u'p',u'q',u'r',u's',u't',u'v',u'w',u'x',u'y',u'z']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a bag-of-words for the tweets, convert the words to lemmas and remove any stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10647     [cutting, celebrety, posting, wound, online, go]\n",
       "6758     [heavy, rain, gusty, wind, vivid, lightning, m...\n",
       "7748     [follow, 4700, block, sw, 11th, st, gresham, g...\n",
       "3438     [dozen, die, two, train, derail, river, indiah...\n",
       "9783     [billionaire, mottas, try, getting, trapped, m...\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_into_lemmas(tweet):\n",
    "    tweet = unicode(tweet, 'utf8').lower()\n",
    "    words = TextBlob(tweet).words\n",
    "    # for each word, take its \"base form\" = lemma \n",
    "    return [word.lemma for word in words if word not in stop]\n",
    "\n",
    "tweets.tweet.head().apply(split_into_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26085\n",
      "Wall time: 5.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "bow_transformer = CountVectorizer(analyzer=split_into_lemmas).fit(tweets['tweet'])\n",
    "print len(bow_transformer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " enews ben affleck i know there s a wife kids and other girls but i can t help it i ve loved him since armageddon eonlinechat\n"
     ]
    }
   ],
   "source": [
    "tweet4 = tweets['tweet'][456]\n",
    "print tweet4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2142)\t1\n",
      "  (0, 2824)\t1\n",
      "  (0, 3627)\t1\n",
      "  (0, 8026)\t1\n",
      "  (0, 8101)\t1\n",
      "  (0, 9813)\t1\n",
      "  (0, 10701)\t1\n",
      "  (0, 13055)\t1\n",
      "  (0, 13210)\t1\n",
      "  (0, 14131)\t1\n",
      "  (0, 20729)\t1\n",
      "  (0, 24807)\t1\n"
     ]
    }
   ],
   "source": [
    "bow4 = bow_transformer.transform([tweet4])\n",
    "print bow4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparse matrix shape: (10860, 26085)\n",
      "number of non-zeros: 117482\n",
      "sparsity: 0.04%\n",
      "Wall time: 5.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tweets_bow = bow_transformer.transform(tweets['tweet'])\n",
    "print 'sparse matrix shape:', tweets_bow.shape\n",
    "print 'number of non-zeros:', tweets_bow.nnz\n",
    "print 'sparsity: %.2f%%' % (100.0 * tweets_bow.nnz / (tweets_bow.shape[0] * tweets_bow.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the tweets into a training and testing set, using the first 8000 tweets for training and the remaining for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 26085)\n",
      "(2860, 26085)\n"
     ]
    }
   ],
   "source": [
    "tweets_bow_train = tweets_bow[:8000]\n",
    "tweets_bow_test = tweets_bow[8000:]\n",
    "tweets_class_train = tweets['class'][:8000]\n",
    "tweets_class_test = tweets['class'][8000:]\n",
    "\n",
    "print tweets_bow_train.shape\n",
    "print tweets_bow_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Multinomial Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23 ms\n"
     ]
    }
   ],
   "source": [
    "%time tweet_class = MultinomialNB().fit(tweets_bow_train, tweets_class_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: Not Relevant\n",
      "expected: Not Relevant\n"
     ]
    }
   ],
   "source": [
    "print 'predicted:', tweet_class.predict(bow4)[0]\n",
    "print 'expected:', tweets['class'][456]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Not Relevant' 'Not Relevant' 'Not Relevant' ..., 'Not Relevant'\n",
      " 'Relevant' 'Not Relevant']\n"
     ]
    }
   ],
   "source": [
    "predictions = tweet_class.predict(tweets_bow_test)\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.805944055944\n",
      "confusion matrix\n",
      "[[1407  255]\n",
      " [ 300  898]]\n",
      "(row=expected, col=predicted)\n"
     ]
    }
   ],
   "source": [
    "print 'accuracy', accuracy_score(tweets_class_test, predictions)\n",
    "print 'confusion matrix\\n', confusion_matrix(tweets_class_test, predictions)\n",
    "print '(row=expected, col=predicted)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "Not Relevant       0.82      0.85      0.84      1662\n",
      "   Relevant       0.78      0.75      0.76      1198\n",
      "\n",
      "avg / total       0.81      0.81      0.81      2860\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(tweets_class_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model with some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat stuck in a tree. [[ 0.68802  0.31198]] ['Not Relevant'] \n",
      "\n",
      "Car accident. Major damage to property. [[ 0.03936  0.96064]] ['Relevant'] \n",
      "\n",
      "I ate a sandwich last night. [[ 0.90025  0.09975]] ['Not Relevant'] \n",
      "\n",
      "Somehow, Mr. Dreyfuss maintains his sound comic timing even when Frank Ozs antic direction calls for hand-waving hysteria. [[ 0.96873  0.03127]] ['Not Relevant'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_tweet(new_tweet): \n",
    "    new_sample = bow_transformer.transform([new_tweet])\n",
    "    print new_tweet, np.around(tweet_class.predict_proba(new_sample), decimals=5), tweet_class.predict(new_sample),\"\\n\"\n",
    "\n",
    "predict_tweet('Cat stuck in a tree.')\n",
    "predict_tweet('Car accident. Major damage to property.')\n",
    "predict_tweet('I ate a sandwich last night.')\n",
    "predict_tweet('Somehow, Mr. Dreyfuss maintains his sound comic timing even when Frank Oz''s antic direction calls for hand-waving hysteria.')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
